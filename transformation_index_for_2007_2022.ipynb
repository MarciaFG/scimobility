{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO98OZ2HToq4WfC+xe3hmWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarciaFG/scimobility/blob/main/transformation_index_for_2007_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformative Flows Project (2007-2022)**\n",
        "**Author:** Marcia R. Ferreira (Complexity Science Hub Vienna & TU Wien)\n",
        "- **Inputs:** \n",
        "\n",
        "1.   CWTS SQL Server [dimensions_2022jun]:\n",
        "\n",
        "\n",
        "      *   Exported File:\n",
        "      *   Exported File:\n",
        "\n",
        "\n",
        "2.   CWTS Publication-level classification system: Meso-fields level [dimensions_2022jun_classification]\n",
        "3.   Dimension reduction-based clustering: Laplacian matrix contructed from meso-field level topic matrix and second eigenvector of the matrix\n",
        "4.   Dimensions database on BigQuery\n",
        "\n",
        "\n",
        "- **Outputs:**"
      ],
      "metadata": {
        "id": "y-DzlwsCdcsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and drivers"
      ],
      "metadata": {
        "id": "vHLeNDGsdvjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Ue500cAdbLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3558f1d9-7952-4dcc-ef67-add05244b253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 13 14:43:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "#!pip install psutil\n",
        "#!pip install humanize\n",
        "#!pip install pynput\n",
        "#pip install plotly==5.4.0\n",
        "!pip install patool\n",
        "\n",
        "# main libraries\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import torch\n",
        "import nltk\n",
        "import GPUtil as GPU\n",
        "\n",
        "# plotting\n",
        "import plotly.graph_objs as go\n",
        "import plotly.io as pio\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.colab import files\n",
        "%load_ext google.colab.data_table\n",
        "%load_ext google.cloud.bigquery\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wQDQL_QEd3xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab312a4-49a4-4a1c-9ebc-a3f1ed9dc90a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7408 sha256=04336840c20cf1011bfcfa5b5630ede460255ac186f5c976ef1125f67aeb1f7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/b5/24/fbb56595c286984f7315ee31821d6121e1b9828436021a88b3\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide your credentials to the runtime\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "# declare your project \n",
        "project_id = \"cshdimensionstest\""
      ],
      "metadata": {
        "id": "5fJiI8ij1Wyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data imports"
      ],
      "metadata": {
        "id": "g44G5rvngOrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" NOT RUN\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1wCFzWEAwBqH47qGQG1_-G6wPgrrs03A6'\n",
        "print(id) # Verify that you have everything\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('second_eigenvector_clustering.csv')  \n",
        "clusters = pd.read_csv('second_eigenvector_clustering.csv', sep=\",\", index_col=0) # Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "print(clusters.head(10))\n",
        "print(\"The data types are as follows:\\n\", clusters.dtypes)\n",
        "print(\"The type of object is:\\n\", type(clusters))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mF7pLxCkfsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" NOT RUN\n",
        "# unzip the files exported from SQL Server\n",
        "#!unzip \"/content/drive/My Drive/TRANSFORMATION/data_export.zip\"\n",
        "#!unzip \"/content/drive/My Drive/TRANSFORMATION/data_export.zip\" > /dev/null\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Qwu5HLlzYbTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import patoolib\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Path of the zip file in Google Drive\n",
        "zip_path = \"/content/drive/My Drive/TRANSFORMATION/data_export.zip\"\n",
        "\n",
        "# Name of the CSV file(s) inside the zip\n",
        "csv_file_names = [  \"spectral_meso_clusters.csv\"\n",
        "                  , \"for_division_labels.csv\"\n",
        "                  , \"grid_ranks.csv\"\n",
        "                  , \"trajectories_au_fourfive_skill.csv\"\n",
        "                  , \"trajectories_au_morethanfive_skill.csv\"\n",
        "                  , \"trajectories_au_single_skill.csv\"\n",
        "                  , \"trajectories_au_twothree_skill.csv\"]\n",
        "\n",
        "# Separator character to use in the CSV files\n",
        "separator = \";\"\n",
        "\n",
        "# Extract the zip file to a temporary directory\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    patoolib.extract_archive(zip_path, outdir=tmpdir)\n",
        "    \n",
        "    # Load each CSV file into its own dataframe\n",
        "    dfs = []\n",
        "    for csv_file_name in csv_file_names:\n",
        "        csv_file_path = os.path.join(tmpdir, csv_file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file_path, sep=separator, encoding='utf-8', header= None, decimal=\".\")\n",
        "            dfs.append(df)\n",
        "        except pd.errors.ParserError:\n",
        "            print(f\"Error loading {csv_file_name}: Skipping...\")\n",
        "\n",
        "# Print the first few rows of each dataframe\n",
        "for i, df in enumerate(dfs):\n",
        "    print(f\"Dataframe {i}:\")\n",
        "    print(df.head(2))\n",
        "print(\"###########################################\")\n",
        "\n",
        "# extract the datasets and store them into a pandas dataframe\n",
        "spectral_meso_clusters = dfs[0]\n",
        "for_division_labels = dfs[1]\n",
        "grid_ranks = dfs[2]\n",
        "trajectories_au_fourfive_skill = dfs[3]\n",
        "trajectories_au_morethanfive_skill = dfs[4]\n",
        "trajectories_au_single_skill = dfs[5]\n",
        "trajectories_au_twothree_skill = dfs[6]\n",
        "\n",
        "print(type(for_division_labels))\n",
        "print(\"###########################################\")"
      ],
      "metadata": {
        "id": "rD6eLoCry8es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the datasets and store them into a pandas dataframe\n",
        "spectral_meso_clusters = dfs[0]\n",
        "for_division_labels = dfs[1]\n",
        "grid_ranks = dfs[2]\n",
        "trajectories_au_fourfive_skill = dfs[3]\n",
        "trajectories_au_morethanfive_skill = dfs[4]\n",
        "trajectories_au_single_skill = dfs[5]\n",
        "trajectories_au_twothree_skill = dfs[6]\n",
        "\n",
        "print(type(for_division_labels))"
      ],
      "metadata": {
        "id": "ZR_R6_anR2IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "pWefljUG1fsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the first row as the header\n",
        "spectral_meso_clusters.columns = spectral_meso_clusters.iloc[0]\n",
        "for_division_labels.columns = for_division_labels.iloc[0]\n",
        "grid_ranks.columns = grid_ranks.iloc[0]\n",
        "\n",
        "# Remove the first row (which is now the header)\n",
        "spectral_meso_clusters = spectral_meso_clusters[1:]\n",
        "for_division_labels = for_division_labels[1:]\n",
        "grid_ranks = grid_ranks[1:]\n",
        "\n",
        "print(spectral_meso_clusters.head())\n",
        "print(for_division_labels.head())\n",
        "print(grid_ranks.head())\n",
        "print(\"###########################################\")\n",
        "\n",
        "def convert_to_float(val):\n",
        "    if isinstance(val, str) and val.replace('.', '', 1).isdigit():\n",
        "        return float(val.replace(',', '.'))\n",
        "    return val\n",
        "\n",
        "# Apply the function to all elements of the dataframe\n",
        "grid_ranks = grid_ranks.applymap(convert_to_float)\n",
        "spectral_meso_clusters = spectral_meso_clusters.applymap(convert_to_float)\n",
        "\n",
        "print(grid_ranks.dtypes)\n",
        "print(spectral_meso_clusters.dtypes)\n",
        "print(\"###########################################\")\n",
        "\n",
        "\n",
        "from pandas.core.dtypes.dtypes import dtypes\n",
        "from numpy.core.multiarray import dtype\n",
        "\n",
        "headers = ['researcher_id', 'grid_id', 'start', 'end', 'Lenght', 'for_division_id', 'meso_field', 'spectral_cluster_id', 'concatenated_fields', 'year', 'n_pubs']\n",
        "\n",
        "# set the new column names using the list\n",
        "trajectories_au_morethanfive_skill.columns = headers\n",
        "trajectories_au_fourfive_skill.columns = headers\n",
        "trajectories_au_single_skill.columns = headers\n",
        "trajectories_au_twothree_skill.columns = headers\n",
        "\n",
        "# print the updated column names\n",
        "print(trajectories_au_morethanfive_skill.columns)\n",
        "print(trajectories_au_morethanfive_skill.dtypes)\n",
        "print(\"###########################################\")\n",
        "\n",
        "print(trajectories_au_morethanfive_skill.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "QHgo0KgkVhU3",
        "outputId": "f6a597ad-1084-4169-bcf5-3fc13328c16f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-45ffcdaca9a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the first row as the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspectral_meso_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectral_meso_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfor_division_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfor_division_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgrid_ranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_ranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spectral_meso_clusters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_org_sequence(df):\n",
        "    # select the desired columns and drop duplicates\n",
        "    df = df[['researcher_id', 'grid_id', 'start', 'end']].drop_duplicates().reset_index(drop=True)\n",
        "    \n",
        "    # calculate the org_sequence using rank()\n",
        "    df['org_sequence'] = df.groupby('researcher_id')['start'].rank(method='dense')\n",
        "    \n",
        "    return df\n",
        "\n",
        "# calculate org_sequence for each dataframe\n",
        "sq_1_skill_df = calculate_org_sequence(trajectories_au_single_skill)\n",
        "sq_2_3_skill_df = calculate_org_sequence(trajectories_au_twothree_skill)\n",
        "sq_4_5_skill_df = calculate_org_sequence(trajectories_au_fourfive_skill)\n",
        "sq_5_or_more_skill_df = calculate_org_sequence(trajectories_au_morethanfive_skill)\n",
        "\n",
        "# merge the two dataframes on researcher_id and grid_id\n",
        "sq_1_skill_df = pd.merge(trajectories_au_single_skill, sq_1_skill_df, on=['researcher_id', 'grid_id'], how='left')\n",
        "sq_1_skill_df = sq_1_skill_df.loc[:, ~sq_1_skill_df.columns.str.endswith('_y')]\n",
        "sq_1_skill_df = sq_1_skill_df.rename(columns=lambda x: x[:-2] if x.endswith('_x') else x)\n",
        "sq_1_skill_df['concatenated_2'] =  sq_1_skill_df['for_division_id'].astype(str) + ' - ' + sq_1_skill_df['spectral_cluster_id'].astype(str)\n",
        "sq_1_skill_df = sq_1_skill_df[['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence', 'n_pubs']].drop_duplicates().reset_index(drop=True)\n",
        "sq_1_skill_df = sq_1_skill_df.groupby(['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence']).sum().reset_index()\n",
        "\n",
        "sq_2_3_skill_df = pd.merge(trajectories_au_twothree_skill, sq_2_3_skill_df, on=['researcher_id', 'grid_id'], how='left')\n",
        "sq_2_3_skill_df = sq_2_3_skill_df.loc[:, ~sq_2_3_skill_df.columns.str.endswith('_y')]\n",
        "sq_2_3_skill_df = sq_2_3_skill_df.rename(columns=lambda x: x[:-2] if x.endswith('_x') else x)\n",
        "sq_2_3_skill_df['concatenated_2'] =  sq_2_3_skill_df['for_division_id'].astype(str) + ' - ' + sq_2_3_skill_df['spectral_cluster_id'].astype(str)\n",
        "sq_2_3_skill_df = sq_2_3_skill_df[['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence', 'n_pubs']].drop_duplicates().reset_index(drop=True)\n",
        "sq_2_3_skill_df = sq_2_3_skill_df.groupby(['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence']).sum().reset_index()\n",
        "\n",
        "sq_4_5_skill_df = pd.merge(trajectories_au_fourfive_skill, sq_4_5_skill_df, on=['researcher_id', 'grid_id'], how='left')\n",
        "sq_4_5_skill_df = sq_4_5_skill_df.loc[:, ~sq_4_5_skill_df.columns.str.endswith('_y')]\n",
        "sq_4_5_skill_df = sq_4_5_skill_df.rename(columns=lambda x: x[:-2] if x.endswith('_x') else x)\n",
        "sq_4_5_skill_df['concatenated_2'] =  sq_4_5_skill_df['for_division_id'].astype(str) + ' - ' + sq_4_5_skill_df['spectral_cluster_id'].astype(str)\n",
        "sq_4_5_skill_df = sq_4_5_skill_df[['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence', 'n_pubs']].drop_duplicates().reset_index(drop=True)\n",
        "sq_4_5_skill_df = sq_4_5_skill_df.groupby(['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence']).sum().reset_index()\n",
        "\n",
        "sq_5_or_more_skill_df = pd.merge(trajectories_au_morethanfive_skill, sq_5_or_more_skill_df, on=['researcher_id', 'grid_id'], how='left')\n",
        "sq_5_or_more_skill_df = sq_5_or_more_skill_df.loc[:, ~sq_5_or_more_skill_df.columns.str.endswith('_y')]\n",
        "sq_5_or_more_skill_df = sq_5_or_more_skill_df.rename(columns=lambda x: x[:-2] if x.endswith('_x') else x)\n",
        "sq_5_or_more_skill_df['concatenated_2'] =  sq_5_or_more_skill_df['for_division_id'].astype(str) + ' - ' + sq_5_or_more_skill_df['spectral_cluster_id'].astype(str)\n",
        "sq_5_or_more_skill_df = sq_5_or_more_skill_df[['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence', 'n_pubs']].drop_duplicates().reset_index(drop=True)\n",
        "sq_5_or_more_skill_df = sq_5_or_more_skill_df.groupby(['researcher_id', 'grid_id', 'concatenated_2', 'org_sequence']).sum().reset_index()\n",
        "\n",
        "print(sq_1_skill_df.head())\n",
        "print(len(sq_1_skill_df))\n",
        "print(len(trajectories_au_single_skill))\n",
        "\n",
        "#print(sq_1_skill_df.head(10))\n",
        "# select all rows that have org_sequence > 1\n",
        "#sq_1_skill_df_filtered = sq_1_skill_df[sq_1_skill_df['org_sequence'] > 1]\n",
        "#print(sq_1_skill_df_filtered.head(10))\n",
        "# select all rows that have researcher_id = 'ur.01000012260.80'\n",
        "#sq_1_skill_df_filtered_au = sq_1_skill_df[sq_1_skill_df['researcher_id'] == 'ur.01000012260.80']\n",
        "#print(sq_1_skill_df_filtered_au.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "rY7dPXli-b2B",
        "outputId": "60a3267b-db14-4270-f52c-49f281b6ccb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-71ba7785ab99>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# calculate org_sequence for each dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msq_1_skill_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_org_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories_au_single_skill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msq_2_3_skill_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_org_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories_au_twothree_skill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msq_4_5_skill_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_org_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories_au_fourfive_skill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trajectories_au_single_skill' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataframe(df, org_seq_df):\n",
        "    # merge the dataframes on researcher_id and grid_id\n",
        "    merged_df = pd.merge(df, org_seq_df, on=['researcher_id', 'grid_id'], how='left')\n",
        "    merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_y')]\n",
        "    merged_df = merged_df.rename(columns=lambda x: x[:-2] if x.endswith('_x') else x)\n",
        "\n",
        "    # concatenate two columns\n",
        "    merged_df['concatenated_2'] = merged_df['for_division_id'].astype(str) + ' - ' + merged_df['spectral_cluster_id'].astype(str)\n",
        "\n",
        "    # select and aggregate columns\n",
        "    selected_cols = ['researcher_id', 'grid_id', 'for_division_id', 'concatenated_2', 'org_sequence', 'n_pubs']\n",
        "    selected_df = merged_df[selected_cols].drop_duplicates().reset_index(drop=True)\n",
        "    aggregated_df = selected_df.groupby(['researcher_id', 'grid_id','for_division_id', 'concatenated_2','org_sequence']).sum().reset_index()\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "sq_1_skill_df = process_dataframe(trajectories_au_single_skill, calculate_org_sequence(trajectories_au_single_skill))\n",
        "sq_2_3_skill_df = process_dataframe(trajectories_au_twothree_skill, calculate_org_sequence(trajectories_au_twothree_skill))\n",
        "sq_4_5_skill_df = process_dataframe(trajectories_au_fourfive_skill, calculate_org_sequence(trajectories_au_fourfive_skill))\n",
        "sq_5_or_more_skill_df = process_dataframe(trajectories_au_morethanfive_skill, calculate_org_sequence(trajectories_au_morethanfive_skill))\n",
        "\n",
        "print(sq_1_skill_df.head())\n",
        "print(len(sq_1_skill_df))\n",
        "print(len(trajectories_au_single_skill))"
      ],
      "metadata": {
        "id": "jHhghZlE2lBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks good!**"
      ],
      "metadata": {
        "id": "AKD-KDlhkmZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similarity scores"
      ],
      "metadata": {
        "id": "A_0bWaHt4Z61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(df):\n",
        "    # Sort the dataframe by researcher_id, org_sequence, and year_published\n",
        "    df = df.sort_values(['researcher_id', 'org_sequence'])\n",
        "    \n",
        "    # Get unique combinations of researcher_id, org_sequence, grid_id, and for_division_id\n",
        "    unique_combinations = df[['researcher_id', 'org_sequence', 'grid_id', 'for_division_id']].drop_duplicates()\n",
        "    \n",
        "    # Create an empty dataframe to store the results\n",
        "    results = pd.DataFrame(columns=['researcher_id', 'prev_org_sequence', 'next_org_sequence', 'prev_grid_id', 'next_grid_id', 'prev_for_division_id','next_for_division_id', 'cosine_similarity'])\n",
        "    \n",
        "    # Iterate over each unique combination of researcher_id, org_sequence, grid_id, and for_division_id\n",
        "    for idx, row in unique_combinations.iterrows():\n",
        "        researcher_id = row['researcher_id']\n",
        "        org_sequence = row['org_sequence']\n",
        "        grid_id = row['grid_id']\n",
        "        for_division_id = row['for_division_id']\n",
        "        \n",
        "        # Filter the dataframe to include only rows with the same researcher_id, org_sequence, grid_id, and for_division_id\n",
        "        filtered_df = df[(df['researcher_id'] == researcher_id) & (df['org_sequence'] == org_sequence) & (df['grid_id'] == grid_id) & (df['for_division_id'] == for_division_id)]\n",
        "        \n",
        "        # Create a pivot table with concatenated_2 as columns, and n_pubs as values\n",
        "        pivot_table = filtered_df.pivot_table(values='n_pubs', index='org_sequence', columns='concatenated_2', aggfunc=np.sum, fill_value=0)\n",
        "        \n",
        "        # Use dynamic programming to calculate the cosine similarity between each adjacent pair of rows in the pivot table\n",
        "        similarity_scores = []\n",
        "        for i in range(len(pivot_table) - 1):\n",
        "            prev_row = pivot_table.iloc[[i]]\n",
        "            next_row = pivot_table.iloc[[i+1]]\n",
        "            cosine_sim = cosine_similarity(prev_row, next_row)[0][0]\n",
        "            similarity_scores.append(cosine_sim)\n",
        "        \n",
        "        # Append the results to the results dataframe\n",
        "        if len(similarity_scores) > 0:\n",
        "            prev_org_sequence = org_sequence - 1\n",
        "            next_org_sequence = org_sequence + 1 if org_sequence < filtered_df['org_sequence'].max() else None\n",
        "            prev_grid_id = filtered_df[filtered_df['org_sequence'] == prev_org_sequence]['grid_id'].values[0] if prev_org_sequence in filtered_df['org_sequence'].values else None\n",
        "            next_grid_id = filtered_df[filtered_df['org_sequence'] == next_org_sequence]['grid_id'].values[0] if next_org_sequence is not None and next_org_sequence in filtered_df['org_sequence'].values else None\n",
        "            result_row = {'researcher_id': researcher_id, 'prev_org_sequence': prev_org_sequence, 'next_org_sequence': next_org_sequence,\n",
        "                          'prev_grid_id': prev_grid_id, 'next_grid_id': next_grid_id, 'prev_for_division_id': prev_for_division_id, 'next_for_division_id': next_for_division_id,\n",
        "                          'cosine_similarity': similarity_scores}\n",
        "            results = results.append(result_row, ignore_index=True)\n",
        "            \n",
        "    # Return the results dataframe\n",
        "    return results\n",
        "calculate_cosine_similarity(sq_1_skill_df)\n"
      ],
      "metadata": {
        "id": "PzJc54_g4Y30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}